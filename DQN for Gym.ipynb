{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "import os\n",
    "from datetime import datetime\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render GIF Exporter\n",
    "\n",
    "This sinnept of code was taken from Dhruv Karthik [work](https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553). From a list of frames we can build an animation in GIF format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    #discount rate\n",
    "        self.epsilon = 1.0  #exploration rate, starting value of epsilon, thanks to epsilon  the agen ranomly decides its action\n",
    "        self.epsilon_min = 0.01 #minimum value of epsilon, so the agent can explore at least this amount\n",
    "        self.epsilon_decay = 0.995 #multiplicative factor (per episode) for decreasing epsilon, so the the number of exploration decrease as the agent feels good while playing\n",
    "        self.learning_rate = 0.001 #step size, how much the neural net learns in each  iteration\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        #Neural Net for Deep-Q learning Model\n",
    "\n",
    "        #Sequential() creates the layers.\n",
    "        model = keras.models.Sequential()\n",
    "        #Input lauer of state size=4 and a Hidden layer with  24 nodes\n",
    "        model.add(keras.layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        #Hidden layer with 24 nodes\n",
    "        model.add(keras.layers.Dense(24, activation='relu'))\n",
    "        #Output layer with # of actions: 2 nodes, left and right\n",
    "        model.add(keras.layers.Dense(self.action_size, activation='linear'))\n",
    "        #Creates the model based on the previous info\n",
    "        model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    #list of previous experiences and observations using memory\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    #how the agent acts\n",
    "    def act(self, state):\n",
    "        #at the beginning the agent select its action randomly (random between 0 and 1)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "    \n",
    "        act_values = self.model.predict(state) # predict the reward value based on the current state\n",
    "        return np.argmax(act_values[0])  # returns action based on predicted reward\n",
    "\n",
    "    #Training with experiences in the memory\n",
    "    def replay(self, batch_size):\n",
    "        #Samples of some experiences and call themm in minibatch\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, targets_f = [], []\n",
    "        #Information extracted from each memory\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            #if donde = true, target = reward\n",
    "            target = reward\n",
    "            if not done:\n",
    "                #predict the future discounted reward, reward + whats comming in the next state\n",
    "                #Q-value returns 2 outputs: one left and one right (maximum is Q-value)\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            #with self.model tries to train itself, make the agent to approximately map the current state to future discounted reward\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target \n",
    "            # Filtering out states and targets for training\n",
    "            states.append(state[0])\n",
    "            targets_f.append(target_f[0])\n",
    "        #the Neural Net is trained with the state and target_f    \n",
    "        history = self.model.fit(np.array(states), np.array(targets_f), epochs=1, verbose=0)\n",
    "        # Keeping track of loss\n",
    "        loss = history.history['loss'][0]\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return loss\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 1000 #number of games for the agent to play\n",
    "episodes_frames = [] # list of frames for each episode\n",
    "file = open(f'{datetime.now().strftime(\"%d-%b-%Y-%H-%M-%S\")}-log.txt', 'w+') # txt file with episodes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1') #gym environment initialized\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "done = False\n",
    "batch_size = 32\n",
    "\n",
    "#Iterative process\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset() #reset state at the begining of each game\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    \n",
    "    episodes_frames.append([]) # Adding an empty list to store episode's frames\n",
    "    \n",
    "    for time in range(300): #time represents each frame of the game\n",
    "        episodes_frames[e].append(env.render(mode=\"rgb_array\"))\n",
    "        #decide action\n",
    "        action = agent.act(state)\n",
    "        #game advances to the nex frame based on the action, reward is 1  for every frame the pole survived\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        #memorize previous information\n",
    "        agent.memorize(state, action, reward, next_state, done)\n",
    "        #next_state becomesthe current state  for next frame\n",
    "        state = next_state\n",
    "        \n",
    "        #done = true when game ends\n",
    "        if done:\n",
    "            log = f'time: {datetime.now().strftime(\"%d-%b-%Y %H:%M:%S\")}, episode: {e + 1}/{EPISODES}, score: {reward}, e: {agent.epsilon:.2}, step: {time}'\n",
    "            print(log)\n",
    "            file.write(log)\n",
    "            break\n",
    "            \n",
    "        if len(agent.memory) > batch_size:\n",
    "            loss = agent.replay(batch_size)\n",
    "                    # Logging training loss every 10 timesteps\n",
    "                if time % 10 == 0:\n",
    "                    log = f'LOSS -> time: {datetime.now().strftime(\"%d-%b-%Y %H:%M:%S\")}, episode: {e + 1}/{EPISODES}, loss: {loss:.4}, step: {time}'\n",
    "                    print(log)\n",
    "                    file.write(log)\n",
    "\n",
    "file.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode, frames in enumerate(episodes_frames):\n",
    "    save_frames_as_gif(frames, filename='render-episode-{episode}.gif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
